\SetKw{State}{state}
\SetKw{Send}{send}
\SetKw{Wait}{wait}
\SetKw{Call}{call}
\SetKw{Return}{return}

\SetKwComment{Comment}{//}{}

\SetKwProg{Function}{function}{}{end}

\SetKwBlock{parallelblk}{Do In Parallel}{end}
\SetKwBlock{atomicblk}{atomic}{end}

\SetKwFunction{atomicAdd}{AtomicAdd}
\SetKwFunction{sortps}{sortByPredecessorSize}
\SetKwFunction{clientSubmit}{Client::SubmitOp}
\SetKwFunction{clientWait}{Client::WaitOp}
\SetKwFunction{leaderSubmit}{Leader::SubmitOpRecv}
\SetKwFunction{replicaCoord}{Replica::coordRequestRecv}
\SetKwFunction{shardMain}{Leader::shardMain}
\SetKwFunction{CR}{Leader::CoordReqRecv}
\SetKwFunction{CRReply}{Leader::CoordRespRecv}
\SetKwFunction{append}{.append}
\SetKwFunction{find}{.find}
\SetKwFunction{popO}{.popOrderedAndStale()}
\SetKwFunction{sleep}{sleep}

% Ideas for improving the design section
% Currently the design section is structured mostly as ‘what’ and does not include a clear ‘why’. Often the ‘why’ comes after the ‘what’ and is mixed up with an alternate design.

%I think a better way of saying this is what ==> why ==> how

% I recommend starting with the ‘why’ and then following with the ‘what’. This makes it much clearer what your design is trying to accomplish. It also allows readers to read something once and understand it, because you tell them the ‘why’/the goal up front and then they can confirm the design that follow actually achieves that goal. With the current structure they read the ‘what’ first and will guess the ‘why’, that makes it more likely they’ll have the wrong ‘why’ in mind and be confused. It also forces them to go back and reread each description after they get to ‘why’ to see if the ‘what’ really achieves that. This makes reading the paper take much longer and similarly means its much more likely they’ll be confused.

% For instance, I would start the design section with the goals you are trying to achieve:
%1. provide \mdl across shards: issue-ordering & ‘suffix-complete failure’
%2. achieve low latency (and define this clearly)
%3. do so in a scalable way

% And then I would articulate the key design insight:Using a two-tier ordering mechanism with a X fast epoch mechanism that does Y well and a A cross-epoch mechanisms that handles B well. or whatever you think it is






% \algnewcommand{\IfThenElse}[3]{% \IfThenElse{<if>}{<then>}{<else>}
%   \algorithmicif\ #1\ \algorithmicthen\ #2\ \algorithmicelse\ #3}

% \algnewcommand{\IfThen}[2]{% \IfThenElse{<if>}{<then>}{<else>}
%   \algorithmicif\ #1\ \algorithmicthen\ #2}

\section{MDL Protocol}
\label{sec:design}
% 1. what are the key features it needs to do. which component of the protocol does that.
% 2. include parts in the motivation that describe why you can't simply do "X". and then in design you can refer to motivation...


To support \mdl, we introduce a new protocol design, \md. This new design ensures the existing guarantees of \sdl such as (1) a total order of all requests that (2) respects real time ordering, as well as a new guaranty that (3) the total order respects per-client invocation-order. In this section we describe our protocol and how each component works to provide these guarantees.

\subsection{Overview}
The full client and shard protocols are specified in Algorithms ~\ref{clientprotocol}, ~\ref{shardprotocol}, and ~\ref{coordinationprotocol}. Our protocol builds atop Raft, which provides \sdl. 

The key insight our protocol is designed around is the observation that if all requests in an execution are sorted in ascending order by the number of outstanding predecessors, or predecessor set size, then a safe total order will always exist. This derives from the guarantee that every cycle in an \md execution would need to contain at least two requests at the same shard where the first one executed has a larger predecessor set size than the second. More details are provided in our complete proof ~\ref{}. 

Since all requests can not be known a priori, we make use of epochs at each shard, and guarantee sorted execution order within each epoch, as well as a safe ordering across epochs. With this approach, we can prune request interleavings that might break \md upfront, rather than running cycle detection and subsequently failing execution. While this unnecessarily prevents some subset of safe interleavings, the latter scales poorly for very large cycles.

\subsection{Per Client Invocation Order}

We use per-client sequence numbers to enforce invocation order at each individual shard. Sequence numbers are assigned at the client library 
and increase monotonically per shard. For example, a client issuing two requests for the first time to different keys that are on different shards will issue two requests both with sequence number 0. The shard leaders can then easily detect when a client's request
has arrived out of order and can buffer it until the necessary predecessor requests intended for the same shard arrive.

To support invocation order across shards, we introduce a message type for inter-shard communication, called a coordination request (CR). 
When each client submits a request to a shard, in parallel it also submits a coordination request to 
the shard of its predecessor. For some request $R$ issued by client C that has sequence number $s$, a predecessor request $P$ is defined as 
the concurrent request that client $C$ issued with sequence number $s-1$. If request $R$ is not concurrent with any other outstanding requests from client $C$, then $P$ is nil.

Requests can only be executed in a given epoch if they have also been \textit{coordinated} by their predecessor, which is signaled when the shard leader of the predecessor responds success to the CR message. Requests with 0 predecessors are vacuously coordinated. Importantly, a shard can only coordinate
a request if the predecessor request itself is also coordinated (and committed). This inductively guarantees that all concurrent requests that were previously invoked
will be coordinated, and thus sorted in their epochs. This is also why clients only need to submit a single CR to the single outstanding predecessor.

% To to get invocation order, there are multiple mechanisms at play
% 1. sequence numbers
% 2. CR requests that are issued by clients
%       -these only get sent to the immediate predecessor (talk about inductive guarantees)
%       -these only get acked if all the other predecessors have been acked too, guaranteeing they are sorted as well. you can only be in an epoch equivalent to or greater than the epochs of your predecessors (not absolute values)

\subsection{Total Order}
A total execution order is guaranteed by ensuring shards sort \textit{ordered} requests by increasing size of predecessor set within each epoch and execute requests in monotonically increasing epoch order. As mentioned before, sorting requests by the number of predecessors ensures a safe total ordering exists. The client library keeps track of the total number of outstanding requests at a given time, which it submits with each request as its predecessor set size. Requests with equal predecessor set sizes are sorted by arrival order to guarantee a stable sort across replicas.

An \textit{ordered} request is one that has been committed and coordinated. \md commits requests with the same mechanism as regular Raft, and it coordinates requests with the use of CR messages sent between shard leaders. Since executing requests now depends on two requirements being fulfilled, commitment as well as coordination, shards keep track of two logs. The first is an execution log containing \textit{ordered} requests that is segmented by epochs, and the second is a buffered log where request entries exist until they are committed (replicated at a majority) and coordinated. While this could be achieved with one log that contains holes and is executed in custom nonlinear order, we choose to break the log into two logs to simplify things and so the guarantees on the execution log remain consistent with Raft. We include a final \textit{ordering} inter-replica agreement round at the end of each epoch to move requests between logs.

\md can only guarantee a safe total ordering across epochs of shards if it executes \textit{ordered} requests. Figure ~\ref{fig:sortedbatchingwrong} shows an execution that does not abide by this constraint, and only sorts committed (but not coordinated) requests within epochs. A cycle arises among all the requests across epoch boundaries, thus the execution does not have a total order and is not multi-dispatch linearizeable. A SUCCESS response for a given request's CR message, which coordinates it, serves as a promise that all predecessors have been sorted at the same or earlier epochs on their respective shards, which provides a total order across epochs of different shards.

\subsubsection{Failure Semantics}
We define \mdl failures as follows: a group of outstanding requests issued by a client will induce a prefix-closed set of success responses followed by a set of failed responses. It is possible that all of the requests succeed, and the set of failed responses will be empty. Importantly, if a request fails, all concurrent requests invoked by the same client with greater sequence numbers will also fail.

The coordination mechanism that we use to ensure correct ordering across epochs also satisfies these specific failure properties. A request can fail in one of two ways: (1) it times out while waiting to be coordinated, which we implement by bounding the number of epochs during which it can exist in the buffered log, or (2) it receives an explicit FAIL response for its CR message. With the inductive guarantees from CR messages, once a request fails it is guaranteed all subsequent requests will also fail.
% Ensure that epcoh increase monotonically
% ensure that each epoch is executed in sorted order
% ensure that across epoch boundaries nothing fishy happens, guaranteed via CR acks
% ensure we have the extra round trip at the end to "order" commands (this is for failure too??)

\subsection{Batching}
\md makes use of batching to increase throughput and amortize the final \textit{ordering} inter-replica round trip across multiple requests in an epoch. Multi-dispatch linearizeable back-end systems expect to experience more load than their single-dispatch linearizeable counterparts, since individual clients can issue many more requests in the former. For example, for $k$ shards, if clients submit on average 10 requests at a time, the logs at shards of \md backends will be about $10/k$ times longer than the logs of \sd shards. Thus batching is a nice way to handle processing of congested shard logs. Moreover, our coordination mechanism is independent across requests from different clients, thus we do not introduce any head-of-line blocking. For requests that arrive later but become coordinated sooner, those can be executed immediately without waiting on the coordination of requests from separate clients.
% Comment that we expect a more congested log at each shard since now there will be fanout from individual clients
% batching is a nice fit since it can exploit this high load
% moreover, ordering a request does not depend on previously arrived requests from independent clients to be ordered, hwich is a nice design that allows each client to see issue order scale with just their behavior, not other clients'.

\subsection{Correctness}
We provide a full proof of correctness in ~\ref{}.

\subsection{Leader Failure}
% what are our failure semantics?


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% Client %%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}
    \State $PID \gets$ unique client ID\\
    \State $\mathcal{L} \gets \{...\}$ \algorithmiccomment{Shard Leaders}\\
    \State $i \gets 0$ \algorithmiccomment{Sequence No Per Shard}\\
    \State $prevReq \gets NULL$\\
    \State $P \gets 0$\\
    \State $m$ \algorithmiccomment{mutex}\\
    \Function{\clientSubmit{Op, K, V}}{
        $seqno := \atomicAdd(i, 1)$\\
        $Req := (Op, K, V, PID, seqno)$\\
        $m.lock()$ \algorithmiccomment{Critical section begins}\\
        $prq := prevReq$\\
        $prevReq \gets Req$\\
        $P++$\\
        $m.unlock()$ \algorithmiccomment{Critical section begins}\\
        \If{$prevReq \neq NULL$}{
            \Send $CoordinationReq(prq, Req)$ to $L_{K-1} \in \mathcal{L}$\\
        }
        \Send $SubmitOp(Req, P)$ to $L_K \in \mathcal{L}$\\        
    }
    \Function{\clientWait{Req}}{
        \Wait receive $SubmitOpReply(V)$ from $L_K \in \mathcal{L}$\\
        $m.lock()$\\
        \If{$prevReq = Req$}{
            $prevReq \gets NULL$\\
        }
        $m.unlock()$\\
        \Return $V$\\
    }
    \caption{MD-Lin Client}
    \label{clientprotocol}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Shard Leader %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}
    \State $Epoch \gets 0$\\
    \State $Timer \gets T$\\
    \State $accepting \gets True$\\
    \State $commitLog \gets []$\\
    \State $orderedLog \gets []$\\
    \State $ClientSeqnoMap \gets [][]$\\
    \Function{\shardMain{}}{
        \Wait until $Timer = 0$\\
        $accepting \gets False$\\
        $batch := []$\\
        \For{$entry \in commitLog$}{
            \If{$entry.committed \land entry.coordinated$}{
                $entry.epoch \gets Epoch$\\
                $batch\append{entry}$\\
            }
        }
        \sortps{batch}\\
        $orderedLog\append{batch}$\\
        \Send $AppendEntries(batch, Epoch)$ to all $r \in \mathcal{R}$\\
        \Wait receive $AppendEntriesSuccess$ from all $r \in Q \in \mathcal{R}$\\
        $commitLog\popO{}$\\
        $\atomicAdd(Epoch, 1)$\\
        $accepting \gets True$\\
        $Timer \gets T$\\
    }
    \Function{\leaderSubmit{Req(Op, K, V, PID, seqno), P}}{
        \If{$ClientSeqnoMap[PID] \neq s$}{
            $buffer_{P}(\{Op, K, V, PID, s, P\})$\\
            \Return\\
        }
        \Wait until $accepting = True$\\
        \For{$entry \in buffer_{PID}$}{
            $commitLog\append{entry}$\\
            $ClientSeqnoMap[PID]++$\\
            \Send $AppendEntries(entry)$ to all $r \in \mathcal{R}$\\
        }
        \Wait receive $AppendEntriesSuccess$ from all $r \in Q \in \mathcal{R}$ for all $entry \in buffer_{PID}$\\
        $entry.committed \gets True$ for all $entry \in buffer_{P}$ that received $Success$\\
    }
    \caption{MD-Lin Shard Leader Main/Submit}
    \label{shardprotocol}
\end{algorithm}

\begin{algorithm}
    \Function{\CR{prq, rq}}{
        \If{$orderedLog\find{prq} \neq NULL$}{
            \algorithmiccomment{prq already ordered}\\
            \Send $CoordinationResp(rq, SUCCESS)$ to $L_k$\\
        }
        $retries := 0$\\
        \While{($(prq \in commitLog \land not coordinated \land committed) \land (prq \notin orderedLog) \lor retries < N$}{
            \sleep(T)\\
            $retries++$\\
        }
        \eIf{$prq \in orderedLog \lor (prq \in commitLog \land coordinated \land committed)$}{
            \Send $CoordResp(rq, SUCCESS)$ to $L_k$\\
        }
        {
            \Send $CoordResp(rq, FAIL)$ to $L_k$\\
        }
        \Send $CoordResp()$\\
    }
    \Function{\CRReply{rq, v}}{
        $index := commitLog\find{rq}$\\
        \If{$index = NULL$}{
            $\sleep{T}$\\
        }
        $index \gets commitLog\find{rq}$\\
        \If{$index = NULL$}{
            \If{$v = SUCCESS$}{
                $commitLog[index].coordinated = True$\\
            }
            {
                $commitLog[index].coordinated = False$\\
            }
        }
        \algorithmiccomment{CR precedes rq arrival, rq will eventually fail}\\
    }
    \caption{MD-Lin Shard Leader Coordination Request Messages}
    \label{coordinationprotocol}
\end{algorithm}