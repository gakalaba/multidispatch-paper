\section{Introduction}
\label{sec:intro}

% \wl{ I like this text:\\
% *** ``Programmers will not need to
% reason about all the interleavings of concurrent operations from an execution with all potential
% interleavings of all other executions. Instead, they will need only reason about their applicationâ€™s
% correctness when run on a Linearizable system with single-dispatch: if their application is correct
% in that setting, it will be correct when run on an md-Linearizability system while also gaining the
% latency benefit of using multi-dispatch''}



Linearizability is one of the most widely used consistency models.
It is what is provided by Paxos~\cite{lamport1998paxos}, RAFT~\cite{ongaro2014raft}, and PBFT~\cite{castro1999pbft} among many others.
Linearizability has the same guarantees as a single machine that processes operations one at a time in the order it receives them over a network.
This makes it a `strong' consistency model that has very intuitive behavior for programmers to reason about.

But Linearizability was defined 36 years ago~\cite{herlihy1990linearizability,herlihy1987linearizability} and thus predates many developments and trends in computing.
One major trend is the use of \textit{multi-dispatch}, i.e., application processes concurrently dispatch multiple operations in an effort to
decrease application latency.
Linearizability specifically disallows this behavior and instead requires \textit{single-dispatch}, where a client process may only have a single outstanding operation at a time.

This mismatch yields two unfortunate possibilities:
If an application is restricted to single-dispatch when run on a Linearizable system, it gets the guarantees of Linearizability but loses out on the latency improvements from concurrency.
On the other hand, if an application issues multiple concurrent operations against a Linearizable system, it gets the latency improvements from concurrency but the guarantees from Linearizability are lost.

This paper introduces \mdllong{} (\mdl{}), a consistency model similar to Linearizability that allows concurrent client operations and requires the system to appear to order them in the same order a client issues them.
\Mdl{} builds on intuition developed by earlier work, such as A-Linearizability introduced by Zookeeper~\cite{hunt2010zookeeper} and session guarantees~\cite{terry1994session}, for intuitively allowing multiple client operations.
\Mdl{} is distinct from this earlier work because it targets providing Linearizability-like consistency for all operations, is formally specified, and introduces suffix-complete failure semantics.
We argue these make it a natural and elegant extension of Linearizability for multi-dispatch.

%* in this paper we modernize linearizability by introducing multi-dispatch linearizability\\
%** linearizability where clients can have many outstanding operations and operations are ordered by the system in the order they are issued\\
%** builds on intuition developed by earlier work such a zookeeper's a-linearizability or the combination of session guarantees and linearizability\\
%** contribution is a formal definition of multi-dispatch linearizability that makes the contract between systems and applications clear\\
%*** first model to precisely capture this for all operations? (unlike a-linearizability)\\
%*** for instance, suffix-failure semantics\\
%** In turn, this formal model allow us to study \mdl\\

New consistency models typically require programmers learn about a new set of potential behaviors from a system and learn how to reason about them correctly.
Instead of requiring this heavy lift from programmers, we allow them to instead reason about Linearizability, which they are familiar with and which is relatively simple to reason about.
This is possible because we identify a sufficient set of conditions for transforming a single-dispatch program, $A$, into a multi-dispatch program $A^\prime$ that we prove is \textit{externally equivalent} to $A$, i.e., external observers cannot tell the difference between $A$ running on a (single-dispatch) Linearizable system and a $A^\prime$ running on a
comparable \mdl{} system.
Thus programmers can specify and reason about their program as they currently do and then apply our simple transformations to take advantage of the latency benefits of \mdl{} while knowing their program will behave in the same way.

%* with the greater power for programmers and the possibility of parallel operations from a single client comes a new responsibility to implement these new constraints in underlying systems

We find that some existing designs provide \mdllong{} for a single shard~\cite{ongaro2014consensus}, but to the best of our knowledge, there are no existing designs that provide it for multiple shards.
There are three challenges in providing \mdl{} across shards:
(1) ensuring operations are ordered across shards in the order the client issued them,
(2) ensuring suffix-complete failure semantics,
and
(3) providing lower end-to-end latency than sequential single-dispatch Linearizable operations.

We present \sys{}, the first protocol to achieve all of these and thus
provide multi-shard \mdl{}.
\sys{} includes two phases: a parallel fault-tolerance phase followed by a sequential coordination phase.
In the fault tolerance phase, operations are replicated via Paxos~\cite{lamport1998paxos} as soon as they arrive at their relevant shards.
In the coordination phase, operations are coordinated and then executed by the shards. An operation is coordinated by the same client's previous operation via coordination requests that are sent directly from shard to shard.
This sequential coordination is key to enforcing both the client issue order and the suffix-complete failure semantics:
a client's later operation will only be ordered and thus executed if the preceding operation has been successfully replicated and coordinated.

The two phases require more messages and result in higher latency for an \textit{individual} operation than an individual single-dispatch operation.
But, they unlock parallelism that yields lower end-to-end application latency as the number of concurrent operations grows.
...

% sdl:
% client -> leader
% leader -> replica
% replica -> leader
% leader -> client
%
% 4 one-way delays with replication per OP.
% 4N latency for N ops

% mdl:
% client -> leader               client -> prev_op_leader (for all ops at once)
% leader -> replica              
% replica -> leader (committed)
%                                prev_op_leader -> leader (coordinated)
%                                prev_op_leader -> leader (coordinated) ... N-1 of these
%                                leader -> replica
%                                replica -> leader
%                                leader -> client
% 4 N latency for 1 op
% 6 + N latency for N ops


To demonstrate the performance benefits offered by \MDL{},
we implement and evaluate \sys{} in a data center environment.
Compared to multi-Paxos~\cite{lamport1998paxos}, we find \sys{}
reduces end-to-end application latency by \textbf{XX\%}. Due to
additional messages, however, this comes at the cost of reduced
maximum throughput (\textbf{XX} ops/sec compared to \textbf{XX}
ops/sec for multi-Paxos). Our evaluation demonstrates how \sys{} can leverage
batching to achieve the best of both worlds, still achieving
\textbf{XX\%} lower latency \true{while matching multi-Paxos's throughput}.

In summary, this paper makes the following contributions:
\begin{itemize}[leftmargin=*]
\item The definition of \mdllong{}.
\item A proof of external equivalence between an application running on \sdl{} and a transformation of it running on \mdl{}.
\item The first protocol for cross-shard \mdl{}: \sys{}.
\item An implementation and evaluation of \sys{} that shows \true{approaches a 75\% reduction in latency for applications compared to a Linearizable baseline as fanout increases.}
\end{itemize}
