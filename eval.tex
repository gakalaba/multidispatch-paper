\onecolumn\twocolumn
\section{Evaluation}
\label{sec:eval}

% These top level graphs are based on raw data and pick some select points.
% If you want to see the full tput-latency graphs, uncomment the section of graphs at the bottom

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% Uniform fanout %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
% Single shard p99 + p50
\begin{figure*}[!htb]
\centering
\subfloat[1 shard p99]{
  \includegraphics[scale=.6]{figs/1shardp99.png}
  \label{fig:1shardp99}
}
\subfloat[1 shard p50]{
  \includegraphics[scale=.6]{figs/1shardp50.png}
  \label{fig:1shardp50}
}
\hspace{0mm}
% 3 shards p99 + p50
\centering
\subfloat[3 shard p99]{
  \includegraphics[scale=.6]{figs/3shardp99.png}
  \label{fig:3shardp99}
}
\subfloat[3 shard p50]{
  \includegraphics[scale=.6]{figs/3shardp50.png}
  \label{fig:3shardp50}
}
\hspace{0mm}
% 9 shards p99 + p50
\centering
\subfloat[9 shard p99]{
  \includegraphics[scale=.6]{figs/9shardp99.png}
  \label{fig:9shardp99}
}
\subfloat[9 shard p50]{
  \includegraphics[scale=.6]{figs/9shardp50.png}
  \label{fig:9shardp50}
}
\caption{p99 and p50 for various shard settings.}
\end{figure*}
\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% 3 shard skew %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!htb]
\centering
\subfloat[skew\xspace0.5]{
  \includegraphics[scale=.2]{figs/3shards_fanout100_skew0.5_8client_CDF.png}
  \label{fig:3shardsskew}
}
\subfloat[skew\xspace0.9]{
  \includegraphics[scale=.2]{figs/3shards_fanout100_skew0.9_8client_CDF.png}
}
\subfloat[skew\xspace1.3]{
  \includegraphics[scale=.2]{figs/3shards_fanout100_skew1.3_8client_CDF.png}
  \label{fig:3shardsskewlast}
}
\caption{3 shard skew.}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% 9 shard skew %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!htb]
\centering
\subfloat[skew\xspace0.5]{
  \includegraphics[scale=.2]{figs/9shards_fanout100_skew0.5_8client_CDF.png}
  \label{fig:9shardsskew}
}
\subfloat[skew\xspace0.9]{
  \includegraphics[scale=.2]{figs/9shards_fanout100_skew0.9_8client_CDF.png}
}
\subfloat[skew\xspace1.3]{
  \includegraphics[scale=.2]{figs/9shards_fanout100_skew1.3_8client_CDF.png}
  \label{fig:9shardsskewlast}
}
\caption{9 shard skew.}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%TODO
This section describes how we evaluate \system's performance handling requests from multi-dispatch clients compared to \mpaxos's performance handling requests from single-dispatch clients. 
Our focus is on measuring latency and throughput of application-level requests, since we aim to improve end-to-end latency for applications.
We also look at how \system scales from single-shard to multi-shard configurations, and how sensitive performance is to skewed workloads. We report on both the p99 and p50 for application-request latency. 
We find p50 a useful metric to look at since our decision to evaluate application-level requests is already end-user facing.
We show that our approach reduces end-to-end latency for application level-requests by up to 75\% and scales with the number of shards. 

\subsection{Experimental Set-up}
All experiments are conducted on Cloudlab's Utah platform ~\ref{}, using m510 machines. 
Our experimental setup consists of a 9-node cluster and 2 client machines. 
Each machine has 1 Intel Xeon-D processor with 8 physical cores running at 2 GHz with hyperthreading enabled, 64 GB of DDR4-2133 RAM, and a 10 Gb/s NIC. 
Shard leader - shard leader, shard leader - shard replica, and client - shard leader latencies are all equal, with roundtrip latency around 150 $\mu$s,. 
Each shard contains 3 replicas, all placed on different physical machines. For multi-sharded experiments we place shard leaders on different physical machines as well. 


% \al{combine experimental set-up and client design. get rid of metrics, workload,
% failures and replace with, approximately, one sentence each in the experimental
% setup or eval intro}

\subsection{Client Design}
We evenly distribute client processes across the 2 client machines. Client processes submit application-level requests. Each application-level request submits $n$ system-level subrequests, where $n$ is equal to the fanout parameter for a given experiment. 

For single-dispatch clients interacting with \mpaxos, only a single system-level subrequest is in-flight at a time, the next subrequest is not issued until the response is received for the preceding subrequest. For multi-dispatch clients interacting with \system, all $n$ system-level subrequests are in-flight at the same time, each is submitted in order and does not wait until the response is received for the preceding subrequest. Each client process is a closed-loop client that submits a single application-level request at a time. The next application-level request is not submitted until all subrequests return. 

In each experiment, we scale the number of clients issueing requests to both systems from 2 to 128 by factors of 2. It's worth noting that since multi-dispatch clients have $n$ requests in-flight at a time, while single-dispatch clients only have 1 request in-flight at a time, \system leaders have to deal with $n$ more requests than \mpaxos at any given time for the same number of clients.

%For a fair comparison between MDL and SDL, we keep the number of outstanding requests sent to each system the same. To achieve this, MDL uses a smaller number of clients $K$ with the specified number $N$ of outstanding requests per client, while SDL uses $K*N$ clients each with 1 outstanding request per client.

% \wl{How does keeping the overall number of requests the same mean the load is the same? We discussed this, you can explain this clearly: keep number of outstanding requests the same in both systems, for mdl this uses a smaller number of clients with the specified number of outstanding requests per client, for sdl its N clients with 1 outstanding request per client.}

\subsection{Workload}
We consider uniform and skewed key distributions to explore a more representative range of realistic workloads. For the latter, we generate keys according to a Zipfian distribution with varying skew values $\theta \in \{0.5, 0.7, 0.9, 1.1, 1.3\}$, and use a keyspace of size 1 million. 

We do not vary the request type at all, all workloads are 100\% writes, since neither our protocol nor basic Paxos implement op-type specific optimizations. 

\subsection{Failures}
We do not consider leader failover in this evaluation. Since failures are rare, and unavoidably degrade performance for most protocols, we only consider the normal mode of operation for \system and \mpaxos. Details about failover specifics in the protocol can be found in ~\ref{sec:design}.

\subsection{Single-Shard}
In this section, we compare the end-to-end application request latency for \system and \mpaxos configured with a single shard. We consider uniform key distribution and vary the fanout parameter as $1$, $10$, $100$, and $1000$. For each fanout value, we show throughput-latency curves for the p99 application-request latency.

While a protocol that provides \mdl could be optimized for a single-shard setting ~\ref{}, we did not change our multi-sharded protocol for a single shard, therefore we still issue coordination messages for each request which the leader must process. The coordination overhead on a single shard is somewhat smaller as compared to the multi-shard setting since there is no leader-leader latency. However, at higher fanouts, the leader is under higher load and must process a large log of buffered requests and each of their coordination messages. We notice a substantial improvement for latencies at fanouts $100$ and $1000$, approaching greater than 2x. This improvement is attenuated by the processing of coordination overhead for each request. While not shown, this improvement is even greater for p90 and p50, where the tail variability of subrequest latency is less pronounced.

Techniques such as batching could be employed to reduce the number of messages sent for each request. For example, the final agreement messages sent after a request is coordinated and committed could be amortized across multiple entries ordered in the log. Such techniques would increase the maximum throughput of \protocol while maintaining gains in end-to-end latency.

\subsection{Multi-Shard}
\label{sec:shards}
In this section, we look at the application-level request latencies for 3-shard and 9-shard configurations, shown in figures ~\ref{fig:3shardp99} -- ~\ref{fig:9shardp50}.

We expect \system to approach a 25\% of the latency of \mpaxos as fanout increases.
As described in section ~\ref{sec:design}, an application-level request of fanout $n$ induces $n-1$ 1-way intershard coordination messages, that must be serialized with predecessor fault-tolerance and coordination. On the other hand, load balancing of large number of requests across multiple shards helps speed the processing time at each leader.

We see the 9 shard configuration approach this lower bound, with p99 latency close to 25\% that of \mpaxos for fanouts $100$ and $1000$. Similarly, we see 3 shards outperform 9 shards due to less leader-leader latency for application requests that issue subrequests across fewer distinct leaders. On way to achieve similar performance for the single shard configuration could be to configure multiple virtual shards on a single physical machine. This would allow the benefits of distirbuting high fanout load while avoiding leader-leader coordination latency overheads.

\subsection{Skew}
Figures ~\ref{fig:3shardsskew} -- ~\ref{fig:3shardsskewlast} and ~\ref{fig:9shardsskew} -- ~\ref{fig:9shardsskewlast} show various skew values for the 3-shard and 9-shard configurations. We look at CDFs for application-level requests with fanout 100 issued by 8 concurrent clients. 

Increasingly higher skew mimics the single shard setting, where \protocol's coordination does not include network latency, but experiences overload sooner.
Overall we don't see much of an impact of skew on end-to-end latency until a high skew value of 1.3, where the tail latency for application-level requests starts increasing at lower percentiles, by about 2x. We notice this degredation begin earlier at around skew 1.1 as well. We expect much of this is due to the high load on the leader of contentious keys.
\begin{comment}
\subsection{MDL with Geo-rep in the Wide Area}
\label{sec:wide}
We show the e2e app. latency for varying inter-shard latency (which we call the wide area **this might be wrong terminology) and inter-replica latency (which we call geo-replication, also might be wrong terminology).

\subsection{Applications on MDL}
\label{sec:apps}

As described in prior sections, we built a tool to automatically transform applications built to interact with SDL backends to interact with MDL backends, maintaining external equivalence. In this section we select 3 representative applications, A1, A2, A3, and show that when transformed with our tool, all 3 see an improvement in e2e latency. We use DeathStar to benchmark the applications.

A1 is an application that ....

A2 is an application that ...

A3 is an application that ...

We expect transformed applications that have a large degree of data parallelism and are read heavy running on MDL backends to see the largest e2e latency improvements over their pre-transformed counterparts running on SDL backends.

Jeff is still looking for these applications at the moment -- it would be good to pick applications that are read heavy and some that are mixed. All should include varying degrees of data parallelism, to show how some improve after the transformation more than others.
\end{comment}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% 1 shard uniform p99 %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!htb]
\centering
\subfloat[Fanout\xspace1]{
  \includegraphics[scale=.15]{figs/1shard_fanout1_p99.png}
}
\subfloat[Fanout\xspace10]{
  \includegraphics[scale=.15]{figs/1shard_fanout10_p99.png}
}
%\hspace{0mm}
\subfloat[Fanout\xspace100]{
  \includegraphics[scale=.15]{figs/1shard_fanout100_p99.png}
}
\subfloat[Fanout\xspace1000]{
  \includegraphics[scale=.15]{figs/1shard_fanout1000_p99.png}
}
\caption{p99 for single shard setting.}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% 3 shard uniform p99 %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!htb]
\centering
\subfloat[Fanout\xspace1]{
  \includegraphics[scale=.15]{figs/3shard_fanout1_p99.png}
}
\subfloat[Fanout\xspace10]{
  \includegraphics[scale=.15]{figs/3shard_fanout10_p99.png}
}
%\hspace{0mm}
\subfloat[Fanout\xspace100]{
  \includegraphics[scale=.15]{figs/3shard_fanout100_p99.png}
}
\subfloat[Fanout\xspace1000]{
  \includegraphics[scale=.15]{figs/3shard_fanout100_p99.png}
}
\caption{p99 for 3 shard setting.}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% 9 shard uniform p99 %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!htb]
\centering
\subfloat[Fanout\xspace1]{
  \includegraphics[scale=.15]{figs/9shard_fanout1_p99.png}
}
\subfloat[Fanout\xspace10]{
  \includegraphics[scale=.15]{figs/9shard_fanout10_p99.png}
}
%\hspace{0mm}
\subfloat[Fanout\xspace100]{
  \includegraphics[scale=.15]{figs/9shard_fanout100_p99.png}
}
\subfloat[Fanout\xspace1000]{
  \includegraphics[scale=.15]{figs/9shard_fanout1000_p99.png}
}
\caption{p99 for 9 shard setting.}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


