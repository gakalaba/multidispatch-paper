\section{\MDL{} Beyond One Shard}
\label{sec:mdl:zookeeper}

There are several existing systems that could be used to implement
\multidispatch{} linearizability on a single shard. In this section,
we briefly describe them and then show how they fail to extend to
multiple shards.

\begin{figure}[!tb]
    \includegraphics[scale=.45]{figs/somet.png}
    \caption{Example where two concurrent client processes each submit two concurrent operations. Keys \textbf{X} and \textbf{Y} are at different shards. It is possible that $R_2$ arrives before $W_1$ at shard \textbf{X}, and $W_2$ arrives before $R_1$ at shard \textbf{Y}. Since \MDL{} requires operations to take effect in invocation order, if $R_1$ returns 1, then $W_1$ must take effect before $R_1$, so $R_2$ must also return 1.}
    \label{fig:concurrentbatches}
\end{figure}

To the best of our knowledge, Zookeeper~\cite{hunt2010zookeeper} is the
only existing work to explicitly account for the possibility of an 
application process to issue multiple operations concurrently in its
consistency model. The authors denote it Asynchronous Linearizability
(or A-Linearizability)~\cite{hunt2010zookeeper}. But despite its name,
A-Linearizability differs from \MDL{} because Zookeeper allows
reads to return stale (i.e., non-linearizable) reads.

\true{To handle concurrent operations from the same process, Zookeeper uses
per-process sequence numbers.} Sequence numbers are assigned at the process 
library and increase monotonically. This provides sufficient information for
the leader to ensure each process's operations are put in its log in an
order consistent with its issue order
(e.g., by queuing operations in sequence-number order).
This technique could be used with existing leader-based replicated-state-machine
protocols~\cite{ongaro2014raft,lamport1998paxos,oki1988vr} to guarantee
\MDL{} on one shard.\footnote{Zookeeper can be forced to
guarantee \MDL{} by issuing a \texttt{sync} operation before each read to prevent it from returning a stale value.}

But these protocols fail to 
guarantee \MDL{} across multiple shards for two reasons: First,
As mentioned, multiple shards
introduces the possibility for independent failures of operations. A first
operation at one shard may fail and need to be retried (e.g., due to
leader failure) while a second operation at another shard succeeds, and
its effects become visible to other processes. If both of these
operations were issued concurrently from the same process, 
this would violate \MDL{}'s suffix-closed failure semantics.

Second, allowing shards to order operations independently can lead to
violations of \MDL{}'s total order guarantee. For example, consider the
case shown in Figure~\ref{fig:concurrentbatches}. Two processes each issue two
operations, one to key $X$ and the other to key $Y$, which reside on different
shards. Process 1 writes $X$ then $Y$, and process 2 reads $Y$ then $X$. Suppose $W_2$ precedes $R_1$ at shard $Y$, causing it to return $Y=1$. Combined with
process 2's issue-order, this implies $R_2$ must be serialized after $W_1$.
But since operations can arrive at the shards in an arbitrary order, $R_2$ may
precede $W_1$ and return $X=0$, violating \MDL{}.

%In the next section, we describe our new protocol, \sys{}, that tackles these
%challenges and guarantees \multidispatch{} linearizability for a multi-shard
%system.
%