\section{Background \& Motivation}

\stale{In this section, we first provide the necessary background on 
distributed applications and the motivating example that we will
use throughout the paper. We then motivate the need for our
new consistency model and system.}

\paragraph{Background and Terminology.}
We consider here the common model of an application that stores its shared state in a separate backend system.
Users interact with the application by sending \textit{requests}, e.g., a HTTP request, to \textit{application processes} that implement the application logic. The application processes are stateless and there can be many of them concurrently processing requests on behalf of many users. Application processes send \textit{operations} to a backend \textit{system} that provides the concurrent application processes with access to shared, persistent state.

\wl{Do we need or want to say anything about processes can exchange messages?}


An application's processing of a user's request will result in some number of operations to the backend system, which we refer to as the request's \textit{fanout}. Our goal is to minimize the \textit{end-to-end latency} of an application processing a user's request. For instance, a user request that result in 100 operations has a fanout of 100 and we are trying to minimize the latency of the totality of these operations.

A system's \textit{consistency model} provides guarantees about the observable return values for a set of operations.
Programmers rely on a system's consistency model while building their applications. By defining the system's set of allowed behaviors, a consistency model permits programmers to ignore the system's implementation and instead focus on ensuring their applications are correct given these behaviors.

\paragraph{Linearizability is a nearly ideal consistency model.}
\textit{Linearizability} is a 'strong' consistency model that the return values for operations to a system are equivalent to some sequential ordering of them and that the ordering respects real-time precedence so that if one operation finishes before another begins the former operation must be ordered before the later~\cite{herlihy1987linearizability, herlihy1990linearizability}.
Its strong guarantees make it simple for application programmers to reason about. It reduces the behavior of a complex, highly concurrent distributed system to the equivalent of a single machine that processes operations one at time (ensured by the sequential ordering) in the order they arrive over the network (ensured by the real-time ordering). 

Linearizability is one of handful of dominant consistency models and is the consistency model provided by many fundamental protocols---e.g., Paxos~\cite{lamport1998paxos}, RAFT~\cite{ongaro2014raft}, PBFT~\cite{castro1999pbft}---and the many foundational systems that implement them---e.g., Chubby~\cite{chubby}, ZippyDB~\cite{zippydb}, etcd~\cite{etcd}.
%ABD~\cite{abd}, primary-backup~\cite{primary-backup}, etc.\@---

\paragraph{Linearizability's single-dispatch slows down applications.}
Despite its many virtues, Linearizability imposes one major restriction on application processes, they must be \textit{single-dispatch}, i.e., they can only have one outstanding operation at a time. Thus while Linearizability permits concurrency between different processes it disallows it within a given process. This restriction was quite reasonable 36 years ago when Linearizability was introduced and \true{accurately captured its target domain of sequential processes running on multiprocessor accessed shared memory}. For modern applications that run in a distributed setting, however, this restriction has major implications for application latency.

\wl{This would be the place to talk about application processing becoming concurrent if we're going to do that.}

Consider an application request with fanout 100, it is required to wait sequentially for each previous operation to finish before it dispatches the next operation. This results in much higher latency than dispatching the operations in parallel. For instance, if each operation takes 5\,ms then single-dispatch results in a latency of 500\,ms compared to the multi-dispatch latency of 5\,ms.

Modern applications can easily have fanouts of 10, 100, or even 1000. For instance, Meta reports that processing a user's Web request can result in thousands of system interactions~\cite{ajoux2015challenges}.

% \wl{It would be great to include an additional example of many system interactions beyond the Facebook paper, especially if it wasn't from a hyperscaler}
% Jeff: I managed to add another in Jeff Dean's Tail at Scale but
% didn't find any from non-hyperscalar.
\wl{Should also cite and mention the dean2013tail paper to support this}.

\wl{What should we say about processing time? Should we mention it and say we ignore it here for ease of exposition? Or should we include it?}



\paragraph{Naive Parallelization is Error-Prone.}
Programmers can get around the single-dispatch limitation of Linearizability by simply dispatching operations concurrently.
This reduces application latency but because it violates the assumptions of Linearizability it means the programmer can no longer rely on its guarantees. Concurrently dispatched operations have no ordering guarantees between them so now the programmer must reason about all the potential interleavings of all the potentially concurrent operations to make sure they are safe. This violates the fundamental purpose of strong consistency models, which is to make the behavior of the system easy to reason about correctness.

\begin{figure*}
\begin{subfigure}[t]{.2\linewidth}
\begin{center}
\begin{tabular}{ c c }
 $p_1$ & $p_2$ \\
 w(a=1) & r(a) \\  
 w(b=1) & r(b)    
\end{tabular}
\caption{}
\end{center}
\end{subfigure}
~
\begin{subfigure}[t]{.4\linewidth}
\begin{center}
\begin{tabular}{ c c }
 $p_3$ & $p_4$ \\
 test\_and\_set(c==0?1) & test\_and\_set(c==0?2) \\  
 test\_and\_set(d==0?1) & test\_and\_set(d==0?2)   
\end{tabular}
\caption{}
\end{center}
\end{subfigure}
~
\begin{subfigure}[t]{.3\linewidth}
\begin{center}
\begin{tabular}{ c c c }
 $p_5$ & $p_6$ & $p_7$ \\
 w(e=1) & w(f=1) & w(g=1) \\  
 r(g) & r(e)    & \textcolor{gray}{r(f)}
\end{tabular}
\caption{}
\end{center}
\end{subfigure}
~
\begin{subfigure}[t]{.2\linewidth}
\begin{center}
\begin{tabular}{ c c }
 $p_8$ & $p_9$ \\
 w(h=1) & r(h) \\  
 \textcolor{gray}{if(r(i) > 2):} & r(j) \\
 w(j=1) & \\
 \textcolor{gray}{else:} & \\
 \textcolor{gray}{w(j=1)}
\end{tabular}
\caption{}
\end{center}
\end{subfigure}
~
\begin{subfigure}[t]{.2\linewidth}
\begin{center}
\begin{tabular}{ c c }
 $p_{10}$ & $p_{11}$ \\
 w(ka=1) & r(ka) \\  
 w(kb=1) & r(kb)    
\end{tabular}
\caption{}
\end{center}
\end{subfigure}
~
\begin{subfigure}[t]{.2\linewidth}
\begin{center}
\begin{tabular}{ c c }
 $p_{12}$ & $p_{13}$ \\
 w(m=1) & r(n) \\  
 w(n=1) & r(m)    
\end{tabular}
\caption{}
\end{center}
\end{subfigure}
\caption{}
\label{fig:naive_breaks_stuff}
\end{figure*}

Figure~\ref{fig:naive_breaks_stuff} shows several simple examples where naive parallelization results in return values that are not possible under Linearizability. 

Look at all these examples where stuff breaks with naive parallelization:
-you could fix the first one or two manually, if you are really smart, but that is fragile as hell
-little changes to the application make things that were correct now wrong
-also include an example where a system change goes from code being correct to incorrect and you don't know because it's not part of the contract!



In the next section, we propose a new \multidispatch{} 
consistency model, \md{}-linearizability, to remedy this 
tension. \Multidispatch{} linearizability explicitly allows 
applications to issue operations concurrently.

% Consequently, it 
% enables applications to achieve lower end-to-end latency. Provided 
% programmers obey a set of rules when parallelizing their applications 
% (described in Section~\ref{sec:}), an application interacting with a 
% multi-issue linearizability system will behave identically to the 
% original application interacting with a linearizable system. Thus, 
% programmers can reap these performance benefits without fear of 
% breaking their applications.
